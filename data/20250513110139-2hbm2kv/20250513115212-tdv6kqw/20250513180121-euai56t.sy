{"ID":"20250513180121-euai56t","Spec":"1","Type":"NodeDocument","Properties":{"id":"20250513180121-euai56t","title":"2018-Performance Modeling and Evaluation of Distributed Deep Learning Frameworks on GPUs","type":"doc","updated":"20250513180131"},"Children":[{"ID":"20250513180128-f2gf24p","Type":"NodeHeading","HeadingLevel":1,"Properties":{"id":"20250513180128-f2gf24p","updated":"20250513180131"},"Children":[{"Type":"NodeText","Data":"摘要"}]},{"ID":"20250513180128-nti4a5x","Type":"NodeParagraph","Properties":{"id":"20250513180128-nti4a5x","updated":"20250513180128"},"Children":[{"Type":"NodeText","Data":"深度学习框架已经被广泛部署在GPU服务器上，用于学术界和工业界的深度学习应用。在训练深度神经网络( deep neural networks，DNNs )时，有许多标准的过程或算法，如卷积和随机梯度下降( Stochastic Gradient Descent，SGD )，但即使在相同的GPU硬件上运行相同的深度模型，不同框架的运行性能也可能不同。在本研究中，我们评估了4个最先进的分布式深度学习框架(即Caffe - MPI、CNTK、MXNet和TensorFlow)在单GPU、多GPU和多节点环境下的运行性能。我们首先建立了使用SGD训练DNN的标准流程的性能模型，然后用三个流行的卷积神经网络(即AlexNet、谷歌公司和ResNet - 50)对这些框架的运行性能进行了基准测试，之后我们分析了哪些因素导致了这四个框架之间的性能差距。通过分析和实验分析，我们发现了可以进一步优化的瓶颈和开销。主要贡献在于所提出的性能模型和分析在算法设计和系统配置方面提供了进一步的优化方向。"}]}]}